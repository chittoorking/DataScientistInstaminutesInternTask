Q1. What algorithm or architecture do you think of which can satisfy the requirement (if any)?
(Analogy will suffice), bonus point (optional) if you can think of some innovative method outside of
present architectures like stacked LSTM, transformers, etc.

Answer:

I think BERT architecture  can satisfy  the requirement.
BERT is basically an Encoder stack of transformer architecture. 
A transformer architecture is an encoder-decoder network that uses self-attention on the encoder side and attention on the decoder side. 
BERTBASE has 12 layers in the Encoder stack while BERTLARGE has 24 layers in the Encoder stack. 


Q2. Any automatic (python script) approach can you think of to label the data for training?
(Donâ€™t overthink, think of simple approaches that you use as human)

Answer:

In supervised machine learning approach, we must know in advance the number of labels the text documents could belong to. 
Then during pre-processing stage we need to label the data in documents manually for training our model accurately. 
For this task, the data can be tagged with POS or NER depending on problem area. 
After some part of the data being labelled we can define the rules or write our own code to label other part of data and keep checking the model by testing with partial designed model.
